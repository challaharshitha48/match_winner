# -*- coding: utf-8 -*-
"""matchwinner.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GLHadZ58Kv75STAzDFwApAJzsM2fziXv
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

df=pd.read_csv('epl_final.csv')
df

cols_to_drop = [
    'Season', 'MatchDate',
    'FullTimeHomeGoals', 'FullTimeAwayGoals',
    'HalfTimeHomeGoals', 'HalfTimeAwayGoals', 'HalfTimeResult'
]

X = df.drop(columns=cols_to_drop, errors='ignore')
print("Shape of features:", X.shape)
print("Shape of target:", y.shape)

print(df.columns)

df.duplicated().sum()

df.isnull().sum()

import pandas as pd
df = pd.read_csv("epl_final.csv")
df = df[['HomeTeam', 'AwayTeam',
       'HomeShots', 'AwayShots', 'HomeShotsOnTarget','FullTimeResult',
       'AwayShotsOnTarget', 'HomeCorners', 'AwayCorners', 'HomeFouls',
       'AwayFouls', 'HomeYellowCards', 'AwayYellowCards', 'HomeRedCards',
       'AwayRedCards']]
df.to_csv("cleaned_dataset_for_matchwinner.csv",index=False)

import pandas as pd
file_path="cleaned_dataset_for_matchwinner.csv"
df=pd.read_csv(file_path)
categorical_cols=df.select_dtypes(include=['object']).columns
print("Categorical Columns",categorical_cols.tolist())

print("Target column dtype:", y.dtype)
print("Unique values in target:", y.unique())

from sklearn.preprocessing import LabelEncoder
df_encoded = df.copy()
for col in df_encoded.select_dtypes(include=['object']).columns:
    if col != 'FullTimeResult':
        le = LabelEncoder()
        df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))
le_y = LabelEncoder()
df_encoded['FullTimeResult'] = le_y.fit_transform(df_encoded['FullTimeResult'])

print("Target mapping (FTR):", dict(zip(le_y.classes_, le_y.transform(le_y.classes_))))
print("\nSample after encoding:\n", df_encoded.head())

df_onehot = df.copy()
df_onehot = pd.get_dummies(df_onehot, columns=['HomeTeam', 'AwayTeam','FullTimeResult'], drop_first=True)
print(df_onehot.shape)
print(df_onehot.head())

import pandas as pd
df = pd.read_csv("cleaned_dataset_for_matchwinner.csv")
num_cols = df.select_dtypes(include=['int64', 'float64']).columns
def remove_outliers(df, cols):
    for col in cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        df = df[(df[col] >= lower) & (df[col] <= upper)]
    return df
df_no_outliers = remove_outliers(df, num_cols)
print("Original shape:", df.shape)
print("After removing outliers:", df_no_outliers.shape)

import matplotlib.pyplot as plt
import numpy as np
for itr in df.columns:
    if df[itr].dtype == np.int64:
        plt.boxplot(df[itr].values)
        plt.xlabel(itr)
        plt.ylabel('Outliers')
        plt.show()

import matplotlib.pyplot as plt
import numpy as np

# 1. only selecting numeric columns that r
num_cols = df_no_outliers.select_dtypes(include=[np.number]).columns.tolist()

# 2. Remove constant columns
cleaned = df_no_outliers[num_cols].loc[:, df_no_outliers[num_cols].apply(pd.Series.nunique) > 1]

# 3. Replace NaN values with 0
cleaned = cleaned.fillna(0)

# 4. Compute correlation
corr = cleaned.corr()

# 5. Plot heatmap
plt.figure(figsize=(14,10))
plt.imshow(corr, cmap="coolwarm", interpolation="nearest")
plt.colorbar()

plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
plt.yticks(range(len(corr.columns)), corr.columns)

for i in range(len(corr.columns)):
    for j in range(len(corr.columns)):
        plt.text(j, i, f"{corr.iloc[i, j]:.2f}",
                 ha="center", va="center", color="black", fontsize=8)

plt.title("Correlation Heatmap (After Outlier Removal)", fontsize=16)
plt.show()

print(df_no_outliers['HomeShots'].corr(df_no_outliers['AwayShots']))

print(df_no_outliers[['HomeShots','AwayShots']].head(20))

import matplotlib.pyplot as plt

plt.scatter(df_no_outliers['HomeShots'], df_no_outliers['AwayShots'])
plt.xlabel("Home Shots")
plt.ylabel("Away Shots")
plt.title("Home vs Away Shots")
plt.show()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['FullTimeResult'] = le.fit_transform(df['FullTimeResult'])

from sklearn.model_selection import train_test_split
X = df_encoded.drop(columns=['FullTimeResult', 'FullTimeHomeGoals', 'FullTimeAwayGoals'], errors='ignore')
y = df_encoded['FullTimeResult']
print("Features shape (X):", X.shape)
print("Target shape (y):", y.shape)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42,stratify=y)

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
y_pred = model.predict(X_test)
print(" Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=le_y.classes_))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

import matplotlib.pyplot as plt
file_path = "cleaned_dataset_for_matchwinner.csv"
df = pd.read_csv(file_path)
result_counts = df['FullTimeResult'].value_counts()
result_counts.plot(kind='bar', color=['#1f77b4', '#ff7f0e', '#2ca02c'])
plt.title('Distribution of Full Time Results')
plt.xlabel('Result')
plt.ylabel('Number of Matches')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

# ----------------------------
# 1. Import Libraries
# ----------------------------
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
#loading th dataset
df = pd.read_csv("/content/drive/MyDrive/cleaned_dataset_for_matchwinner.csv")
#dropping the unwanted cols
drop_cols = ['Season', 'MatchDate',
             'FullTimeHomeGoals', 'FullTimeAwayGoals',
             'HalfTimeHomeGoals', 'HalfTimeAwayGoals', 'HalfTimeResult']

df_cleaned = df.drop(columns=drop_cols, errors='ignore')

# encoding the target
target_col = 'FullTimeResult'
df_encoded = df_cleaned.copy()

# Encode categorical columns except target
for col in df_encoded.select_dtypes(include=['object']).columns:
    if col != target_col:
        le = LabelEncoder()
        df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))

# Encode target
le_y = LabelEncoder()
df_encoded[target_col] = le_y.fit_transform(df_encoded[target_col])
#split data
X = df_encoded.drop(columns=[target_col])
y = df_encoded[target_col]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
#  Default Random Forest

rf_default = RandomForestClassifier(random_state=42, class_weight="balanced")
rf_default.fit(X_train, y_train)

y_pred_default = rf_default.predict(X_test)

print("\n=== Default Random Forest ===")
print("Accuracy:", accuracy_score(y_test, y_pred_default))
print("\nClassification Report:\n", classification_report(y_test, y_pred_default, target_names=le_y.classes_))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_default))
#  Hyperparameter Tuning with GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 'log2']
}

rf = RandomForestClassifier(random_state=42, class_weight="balanced")

grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print("\n=== Best Hyperparameters (Random Forest) ===")
print(grid_search.best_params_)
# Tuned Random Forest

rf_best = grid_search.best_estimator_
y_pred_best = rf_best.predict(X_test)

print("\n=== Tuned Random Forest ===")
print("Accuracy:", accuracy_score(y_test, y_pred_best))
print("\nClassification Report:\n", classification_report(y_test, y_pred_best, target_names=le_y.classes_))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_best))
import pickle

# Save the trained Random Forest model
with open("/content/drive/MyDrive/match_winner_model.pkl", "wb") as f:
    pickle.dump(rf_best, f)

# Save the label encoder for target column
#with open("/content/drive/MyDrive/le_target.pkl", "wb") as f:
 #   pickle.dump(le_y, f)

print("✅ Model saved successfully!")

from sklearn.utils import resample

# Combine X_train & y_train
train_data = pd.concat([X_train, y_train], axis=1)

# Separate classes
class_counts = y_train.value_counts()
print("Class distribution before balancing:\n", class_counts)

df_majority = train_data[train_data[target_col] == y_train.mode()[0]]
balanced_list = [df_majority]

# Oversample minority classes
for cls in y_train.unique():
    df_class = train_data[train_data[target_col] == cls]
    df_class_upsampled = resample(
        df_class,
        replace=True,
        n_samples=len(df_majority),
        random_state=42
    )
    balanced_list.append(df_class_upsampled)

train_balanced = pd.concat(balanced_list)
X_train_bal = train_balanced.drop(columns=[target_col])
y_train_bal = train_balanced[target_col]

# Retrain
rf_bal = RandomForestClassifier(random_state=42)
rf_bal.fit(X_train_bal, y_train_bal)
y_pred_bal = rf_bal.predict(X_test)

print("\n=== With Oversampling (SMOTE-like) ===")
print("Accuracy:", accuracy_score(y_test, y_pred_bal))
print("\nClassification Report:\n", classification_report(y_test, y_pred_bal, target_names=le_y.classes_))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_bal))

# 1. Model train chesi fit cheyyali
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(X_train, y_train)   # 👈 ikkadike train cheyyali

# 2. Pickle library import cheyyi (pip install avasaram ledu)
import pickle

# 3. Model ni save cheyyi
with open("match_winner_model.pkl", "wb") as f:
    pickle.dump(model, f)

# 4. Tarvata download cheyyi
from google.colab import files
files.download("match_winner_model.pkl")

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ==============================
# 1. Prepare Data (SMOTE + Scale)
# ==============================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

scaler = StandardScaler()
X_train_res = scaler.fit_transform(X_train_res)
X_test = scaler.transform(X_test)

# ==============================
# 2. Function to build models
# ==============================
def build_nn(model_type, input_dim, output_dim):
    model = Sequential()

    if model_type == "shallow":
        model.add(Dense(128, input_dim=input_dim, activation='relu'))
        model.add(Dropout(0.3))
        model.add(Dense(output_dim, activation='softmax'))

    elif model_type == "deep":
        model.add(Dense(512, input_dim=input_dim, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.4))
        model.add(Dense(256, activation='relu'))
        model.add(Dropout(0.3))
        model.add(Dense(128, activation='relu'))
        model.add(Dropout(0.2))
        model.add(Dense(output_dim, activation='softmax'))

    elif model_type == "wide":
        model.add(Dense(1024, input_dim=input_dim, activation='relu'))
        model.add(Dropout(0.5))
        model.add(Dense(512, activation='relu'))
        model.add(Dropout(0.4))
        model.add(Dense(output_dim, activation='softmax'))

    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# ==============================
# 3. Train & Evaluate
# ==============================
callbacks = [
    EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True),
    ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=5, verbose=1)
]

models = ["shallow", "deep", "wide"]
results = {}

for m in models:
    print(f"\n=== Training {m.upper()} NN ===")
    nn = build_nn(m, X_train_res.shape[1], len(le_y.classes_))
    nn.fit(
        X_train_res, y_train_res,
        validation_data=(X_test, y_test),
        epochs=100,
        batch_size=32,
        callbacks=callbacks,
        verbose=0
    )
    y_pred = nn.predict(X_test).argmax(axis=1)
    acc = accuracy_score(y_test, y_pred)
    results[m] = acc
    print(f"{m.upper()} Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred, target_names=le_y.classes_))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# ==============================
# 4. Ensemble (Average Predictions)
# ==============================
print("\n=== Ensemble of All NNs ===")
preds = []
for m in models:
    nn = build_nn(m, X_train_res.shape[1], len(le_y.classes_))
    nn.fit(
        X_train_res, y_train_res,
        validation_data=(X_test, y_test),
        epochs=50,
        batch_size=32,
        verbose=0
    )
    preds.append(nn.predict(X_test))

avg_pred = np.mean(preds, axis=0).argmax(axis=1)
ensemble_acc = accuracy_score(y_test, avg_pred)
print(f"Ensemble Accuracy: {ensemble_acc:.4f}")
print(classification_report(y_test, avg_pred, target_names=le_y.classes_))
print("Confusion Matrix:\n", confusion_matrix(y_test, avg_pred))

# ==============================
# 0. Install packages
# ==============================
!pip install imbalanced-learn xgboost lightgbm -q

# ==============================
# 1. Import Libraries
# ==============================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.utils import resample

# ==============================
# 2. Mount Google Drive
# ==============================
from google.colab import drive
drive.mount('/content/drive')

file_path = "/content/drive/MyDrive/Matchwinner/cleaned_dataset_for_matchwinner.csv"

# ==============================
# 3. Load Dataset
# ==============================
df = pd.read_csv(file_path)

drop_cols = ['Season', 'MatchDate',
             'FullTimeHomeGoals', 'FullTimeAwayGoals',
             'HalfTimeHomeGoals', 'HalfTimeAwayGoals', 'HalfTimeResult']

df_cleaned = df.drop(columns=drop_cols, errors='ignore')
target_col = 'FullTimeResult'

# Encode categorical features
for col in df_cleaned.select_dtypes(include=['object']).columns:
    if col != target_col:
        le = LabelEncoder()
        df_cleaned[col] = le.fit_transform(df_cleaned[col].astype(str))

# Encode target
le_y = LabelEncoder()
df_cleaned[target_col] = le_y.fit_transform(df_cleaned[target_col])

X = df_cleaned.drop(columns=[target_col])
y = df_cleaned[target_col]

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ==============================
# 4. Manual Oversampling (like your old code)
# ==============================
train_data = pd.concat([X_train, y_train], axis=1)
majority_class = y_train.mode()[0]
df_majority = train_data[train_data[target_col] == majority_class]
balanced_list = [df_majority]

for cls in y_train.unique():
    df_class = train_data[train_data[target_col] == cls]
    df_class_upsampled = resample(df_class,
                                  replace=True,
                                  n_samples=len(df_majority),
                                  random_state=42)
    balanced_list.append(df_class_upsampled)

train_balanced = pd.concat(balanced_list)
X_train_bal = train_balanced.drop(columns=[target_col])
y_train_bal = train_balanced[target_col]

# Scaling for NN
scaler = StandardScaler()
X_train_bal_scaled = scaler.fit_transform(X_train_bal)
X_test_scaled = scaler.transform(X_test)

# ==============================
# 5. Train Models
# ==============================
results = {}

# --- Random Forest ---
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_bal, y_train_bal)
y_pred_rf = rf.predict(X_test)
results["Random Forest"] = accuracy_score(y_test, y_pred_rf)

print("\n=== Random Forest ===")
print("Accuracy:", results["Random Forest"])
print(classification_report(y_test, y_pred_rf, target_names=le_y.classes_))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

# --- XGBoost ---
xgb = XGBClassifier(random_state=42, eval_metric="mlogloss", use_label_encoder=False)
xgb.fit(X_train_bal, y_train_bal)
y_pred_xgb = xgb.predict(X_test)
results["XGBoost"] = accuracy_score(y_test, y_pred_xgb)

print("\n=== XGBoost ===")
print("Accuracy:", results["XGBoost"])
print(classification_report(y_test, y_pred_xgb, target_names=le_y.classes_))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))

# --- LightGBM ---
lgb = LGBMClassifier(random_state=42)
lgb.fit(X_train_bal, y_train_bal)
y_pred_lgb = lgb.predict(X_test)
results["LightGBM"] = accuracy_score(y_test, y_pred_lgb)

print("\n=== LightGBM ===")
print("Accuracy:", results["LightGBM"])
print(classification_report(y_test, y_pred_lgb, target_names=le_y.classes_))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lgb))

# --- Neural Network ---
nn = Sequential([
    Dense(256, input_dim=X_train_bal_scaled.shape[1], activation='relu'),
    BatchNormalization(),
    Dropout(0.4),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(len(le_y.classes_), activation='softmax')
])

nn.compile(optimizer=Adam(learning_rate=0.001),
           loss='sparse_categorical_crossentropy',
           metrics=['accuracy'])

early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

nn.fit(X_train_bal_scaled, y_train_bal,
       validation_data=(X_test_scaled, y_test),
       epochs=100, batch_size=32,
       callbacks=[early_stop], verbose=0)

y_pred_nn = nn.predict(X_test_scaled).argmax(axis=1)
results["Neural Network"] = accuracy_score(y_test, y_pred_nn)

print("\n=== Neural Network ===")
print("Accuracy:", results["Neural Network"])
print(classification_report(y_test, y_pred_nn, target_names=le_y.classes_))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_nn))

# ==============================
# 6. Final Accuracy Comparison
# ==============================
print("\n=== Final Accuracy Comparison ===")
for model, acc in results.items():
    print(f"{model}: {acc:.4f}")

!pip install streamlit pandas numpy scikit-learn pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import numpy as np
# import pickle
# 
# # ------------------------------
# # Load trained model
# # ------------------------------
# model = pickle.load(open("match_winner_model.pkl", "rb"))
# 
# # ------------------------------
# # Team encoding (must match training LabelEncoder)
# # ------------------------------
# team_mapping = {
#     "Arsenal": 0,
#     "Chelsea": 1,
#     "Liverpool": 2,
#     "Manchester City": 3,
#     "Default": 0
# }
# 
# # ------------------------------
# # Streamlit UI
# # ------------------------------
# st.title("⚽ Match Winner Predictor")
# st.write("Enter match stats to predict the winner (Home Win / Draw / Away Win).")
# 
# # Teams
# home_team = st.selectbox("Home Team", list(team_mapping.keys()))
# away_team = st.selectbox("Away Team", list(team_mapping.keys()))
# 
# # Create two columns for home and away stats
# col1, col2 = st.columns(2)
# 
# with col1:
#     st.subheader("Home Team Stats")
#     home_shots = st.number_input("Shots", min_value=0, value=10, key="home_shots")
#     home_shots_on_target = st.number_input("Shots on Target", min_value=0, value=5, key="home_shots_target")
#     home_corners = st.number_input("Corners", min_value=0, value=6, key="home_corners")
#     home_fouls = st.number_input("Fouls", min_value=0, value=12, key="home_fouls")
#     home_yellow = st.number_input("Yellow Cards", min_value=0, value=2, key="home_yellow")
#     home_red = st.number_input("Red Cards", min_value=0, value=0, key="home_red")
# 
# with col2:
#     st.subheader("Away Team Stats")
#     away_shots = st.number_input("Shots", min_value=0, value=8, key="away_shots")
#     away_shots_on_target = st.number_input("Shots on Target", min_value=0, value=4, key="away_shots_target")
#     away_corners = st.number_input("Corners", min_value=0, value=5, key="away_corners")
#     away_fouls = st.number_input("Fouls", min_value=0, value=14, key="away_fouls")
#     away_yellow = st.number_input("Yellow Cards", min_value=0, value=3, key="away_yellow")
#     away_red = st.number_input("Red Cards", min_value=0, value=1, key="away_red")
# 
# # Encode teams
# home_team_encoded = team_mapping.get(home_team, 0)
# away_team_encoded = team_mapping.get(away_team, 0)
# 
# # Prepare input for model (14 features)
# input_data = np.array([[home_team_encoded, away_team_encoded,
#                         home_shots, away_shots,
#                         home_shots_on_target, away_shots_on_target,
#                         home_corners, away_corners,
#                         home_fouls, away_fouls,
#                         home_yellow, away_yellow,
#                         home_red, away_red]])
# 
# # Prediction
# if st.button("Predict Winner"):
#     prediction = model.predict(input_data)[0]
# 
#     if prediction == 1:
#         st.success(f"🏠 Prediction: {home_team} Wins")
#     elif prediction == 0:
#         st.info("🤝 Prediction: Match Draw")
#     else:
#         st.error(f"🚩 Prediction: {away_team} Wins")
#

from pyngrok import ngrok, conf
conf.get_default().auth_token = "33SBM8rLwcMx35IVpLxYI2k0L5W_7r1J3aCCPPFApp7hzvJ6n"

import os
from pyngrok import ngrok

# Start Streamlit
os.system("streamlit run app.py --server.port 8501 &")

# Get public URL
public_url = ngrok.connect(8501)
print("Your Dashboard URL:", public_url)